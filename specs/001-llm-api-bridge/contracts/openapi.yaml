openapi: 3.1.0
info:
  title: RemoteLLMconnector API
  description: |
    OpenAI-compatible API exposed through the LLM API Bridge.
    This contract defines the API that external users interact with.
    The connector forwards these requests transparently to the local LLM.
  version: 1.0.0
  license:
    name: MIT

servers:
  - url: https://api.example.com/v1
    description: External API provider endpoint (user-facing)

security:
  - ApiKeyAuth: []

paths:
  /chat/completions:
    post:
      operationId: createChatCompletion
      summary: Create a chat completion
      description: |
        Creates a completion for the provided chat conversation.
        Supports both streaming and non-streaming responses.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/ChatCompletionRequest'
      responses:
        '200':
          description: Successful completion
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ChatCompletionResponse'
            text/event-stream:
              schema:
                type: string
                description: Server-sent events stream for streaming responses
        '401':
          description: Invalid or missing API key
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
        '429':
          description: Rate limit exceeded
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
        '502':
          description: Local LLM unavailable
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
        '504':
          description: Request timeout
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'

  /models:
    get:
      operationId: listModels
      summary: List available models
      description: Returns a list of models available on the local LLM server.
      responses:
        '200':
          description: List of models
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ModelList'
        '401':
          description: Invalid or missing API key
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'

components:
  securitySchemes:
    ApiKeyAuth:
      type: apiKey
      in: header
      name: Authorization
      description: 'Bearer token: `Authorization: Bearer <api_key>`'

  schemas:
    ChatCompletionRequest:
      type: object
      required:
        - model
        - messages
      properties:
        model:
          type: string
          description: ID of the model to use
          example: "llama3.2"
        messages:
          type: array
          description: List of messages in the conversation
          items:
            $ref: '#/components/schemas/Message'
        temperature:
          type: number
          minimum: 0
          maximum: 2
          default: 1
          description: Sampling temperature
        top_p:
          type: number
          minimum: 0
          maximum: 1
          default: 1
          description: Nucleus sampling parameter
        max_tokens:
          type: integer
          minimum: 1
          description: Maximum tokens to generate
        stream:
          type: boolean
          default: false
          description: Whether to stream the response
        stop:
          oneOf:
            - type: string
            - type: array
              items:
                type: string
          description: Stop sequences

    Message:
      type: object
      required:
        - role
        - content
      properties:
        role:
          type: string
          enum: [system, user, assistant]
          description: Role of the message author
        content:
          type: string
          description: Content of the message

    ChatCompletionResponse:
      type: object
      properties:
        id:
          type: string
          description: Unique identifier for the completion
        object:
          type: string
          enum: [chat.completion]
        created:
          type: integer
          description: Unix timestamp of creation
        model:
          type: string
          description: Model used for completion
        choices:
          type: array
          items:
            $ref: '#/components/schemas/Choice'
        usage:
          $ref: '#/components/schemas/Usage'

    Choice:
      type: object
      properties:
        index:
          type: integer
        message:
          $ref: '#/components/schemas/Message'
        finish_reason:
          type: string
          enum: [stop, length, content_filter]

    Usage:
      type: object
      properties:
        prompt_tokens:
          type: integer
        completion_tokens:
          type: integer
        total_tokens:
          type: integer

    ModelList:
      type: object
      properties:
        object:
          type: string
          enum: [list]
        data:
          type: array
          items:
            $ref: '#/components/schemas/Model'

    Model:
      type: object
      properties:
        id:
          type: string
          description: Model identifier
        object:
          type: string
          enum: [model]
        created:
          type: integer
        owned_by:
          type: string

    Error:
      type: object
      required:
        - error
      properties:
        error:
          type: object
          properties:
            message:
              type: string
              description: Human-readable error message
            type:
              type: string
              description: Error type
            code:
              type: string
              description: Error code
